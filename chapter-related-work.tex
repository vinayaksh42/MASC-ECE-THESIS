%======================================================================
\chapter{Related Work}\label{related}
%======================================================================
While much program analysis research considers a single version of a software artifact, some related work treats changes between versions, and we discuss the related work in that area. We also discuss empirical efforts to detect and empirically survey the prevalence of and reasons for breaking changes.

Logozzo et al~\cite{logozzo14:_verif_modul_version} proposed the concept of verification modulo versions. Like us, verification modulo versions observes that program verification needs to recognize that software evolves over time and that verification tools must take this into account---in particular, a developer often wants to know about potential verification issues unique to new code, rather than re-triaging issues previously reported. A fundamental difference between their work and ours is that we put the interface between the client and the library at the centre of our approach, and ensure that changes in the library must be visible to the client before we report them, while the verification modulo versions approach aims to detect behavioural differences between two versions of some software.

Møller et al~\cite{møller20:_detec_locat_javas_progr_affec} propose a domain-specific language for JavaScript library developers to use to indicate to client developers what has changed in a new version of their library. Our work addresses a specific subset of the breaking changes problem but automatically deduces changes in the library that are relevant to a particular client. It does not require additional work on the part of the library developer. More generally, and at the same time, Lam et al~\cite{lam20:_puttin_seman_seman_version} proposed the development of semantic version calculators, including the usage of both traditional and lightweight contracts for libraries, to allow library developers to declare, and client developers to understand, the impact of potential breaking changes in libraries.

Jayasuriya et al~\cite{jayasuriya23:_under_break_chang_wild,jayasuriya24} investigate the prevalence of breaking changes in the wild. In principle, under semantic versioning~\cite{preston-werner23:_seman_version}, library developers ought to indicate breaking changes by incrementing the major version number (i.e. the first number in the version triplet); however, Jayasuriya et al found that 41.58\% of (syntactic) breaking changes were not identified as such, and that 11.58\% of changes were beaking.

Chenguang et al.~\cite{CompCheck} propose a tool, CompCheck, for detecting behavioural breaking changes. It uses both static and dynamic techniques to identify such changes. The approach begins by locating test cases that pass for the current version of the library used by the client but fail after the library is upgraded. CompCheck relies heavily on the presence of existing test cases to detect behavioural breaking changes. It then extracts the usage pattern of the \gls{api} in the failing test case and uses this pattern to identify other clients who use the same library in a similar manner. Here, the original client refers to the client whose test case fails after the upgrade. CompCheck generates a new test case for each newly identified client using EvoSuite~\cite{Gordon2011evosuite}. In contrast, our tool does not rely on existing test cases to discover behavioural breaking changes caused by newly added exceptions. It also does not depend on tools like EvoSuite for test generation. Instead, we use taint analysis to filter out newly added exceptions that the client cannot trigger.

We have proposed a static approach to detecting breaking changes. Mujahid et al~\cite{mujahid20:_using_other_tests_ident_break_updat} proposed a dynamic approach to this problem. Their goal is to answer the question of whether a new version includes breaking changes or not, and they combine tests from ``the crowd'' (a collection of other projects) to decide the question, finding that such tests found breaking changes 60\% of the time. Our approach is much more specific to a particular library/client pair, and aims to detect if library $X$'s upgrade may break client $Y$. More like us, Jayasuriya et al~\cite{jayasuriya24:_under_apis} also use a dynamic approach (compared to our static approach) on a client/library pair to detect behavioural breaking changes in the client using its tests, finding that 2.30\% of library updates broke the client, as witnessed by a particular test.

In terms of better understanding why breaking changes exist, Kong et al~\cite{kong25:_towar_better_compr_break_chang_npm_ecosy} analyzed the reasons that library developers introduced breaking changes (reducing code redundancy, improving identifier names, and improving API design) and proposed a taxonomy of types of changes.
